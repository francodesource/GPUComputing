{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZYqN0UwVLC_"
   },
   "source": [
    "---\n",
    "# **LAB 4 - CUDA memories**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ñ∂Ô∏è CUDA tools..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan 28 14:39:10 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   43C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n",
      "0.60.0\n",
      "Found 1 CUDA devices\n",
      "id 0             b'Tesla T4'                              [SUPPORTED]\n",
      "                      Compute Capability: 7.5\n",
      "                           PCI Device ID: 4\n",
      "                              PCI Bus ID: 0\n",
      "                                    UUID: GPU-a93bfef5-482c-e458-7745-616de7a3cd83\n",
      "                                Watchdog: Disabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "Summary:\n",
      "\t1/1 devices are supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "from numba import cuda\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(np.__version__)\n",
    "print(numba.__version__)\n",
    "\n",
    "cuda.detect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Numba deprecation and performance warnings\n",
    "from numba.core.errors import NumbaDeprecationWarning, NumbaPerformanceWarning\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=NumbaPerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils for compiling and running Numba CUDA code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device object repr: <CUDA device 0 'b'Tesla T4''>\n",
      "Device name:           b'Tesla T4'\n",
      "Compute capability:    (7, 5)\n",
      "  total_memory (bytes)                  : 15828320256\n",
      "  multi_processor_(SM)_count            : 40\n",
      "  max_threads_per_block                 : 1024\n",
      "  max_block_dim_x                       : 1024\n",
      "  max_block_dim_y                       : 1024\n",
      "  max_block_dim_z                       : 64\n",
      "  max_grid_dim_x                        : 2147483647\n",
      "  max_grid_dim_y                        : 65535\n",
      "  max_grid_dim_z                        : 65535\n",
      "  max_shared_memory_per_block (bytes)   : 49152\n",
      "  max_shared_memory_per_SM (bytes)      : 65536\n",
      "  warp_size                             : 32\n",
      "  compute_capability                    : None\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "\n",
    "def mem_snapshot(print=True):\n",
    "    free, total = cuda.current_context().get_memory_info()\n",
    "    used = total - free\n",
    "    if print:\n",
    "        # print GPU memory info\n",
    "        print(\"\\nMemory occupancy:\")\n",
    "        print(f\"    GPU total: {total/1024**3:.3f} GB\")\n",
    "        print(f\"    GPU free : {free/1024**3:.3f} GB\")\n",
    "        print(f\"    GPU used : {used/1024**3:.3f} GB\")\n",
    "    else:\n",
    "        return used, free, total\n",
    "\n",
    "# Quick device spec report (Numba)\n",
    "def device_info(show=True):\n",
    "    dev = cuda.get_current_device()   # raises if no CUDA device\n",
    "    _, total = cuda.current_context().get_memory_info() \n",
    "    \n",
    "    if show:\n",
    "        print(\"Device object repr:\", dev)\n",
    "        print(\"Device name:          \", getattr(dev, \"name\", \"<unknown>\"))\n",
    "        print(\"Compute capability:   \", getattr(dev, \"compute_capability\", \"<unknown>\"))\n",
    "\n",
    "    # Common numeric properties (use getattr to avoid attribute errors)\n",
    "    props = {\n",
    "        \"  multi_processor_(SM)_count\": [\"MULTIPROCESSOR_COUNT\"],\n",
    "        \"  max_threads_per_block\": [\"MAX_THREADS_PER_BLOCK\"],\n",
    "        \"  max_block_dim_x\":       [\"MAX_BLOCK_DIM_X\"],\n",
    "        \"  max_block_dim_y\":       [\"MAX_BLOCK_DIM_Y\"],\n",
    "        \"  max_block_dim_z\":       [\"MAX_BLOCK_DIM_Z\"],\n",
    "        \"  max_grid_dim_x\":        [\"MAX_GRID_DIM_X\"],\n",
    "        \"  max_grid_dim_y\":        [\"MAX_GRID_DIM_Y\"],\n",
    "        \"  max_grid_dim_z\":        [\"MAX_GRID_DIM_Z\"],\n",
    "        \"  max_shared_memory_per_block (bytes)\": [\"MAX_SHARED_MEMORY_PER_BLOCK\"],\n",
    "        \"  max_shared_memory_per_SM (bytes)\": [\"MAX_SHARED_MEMORY_PER_MULTIPROCESSOR\"],\n",
    "        \"  warp_size\":             [\"WARP_SIZE\"],\n",
    "        \"  compute_capability\":    [\"COMPUTE_CAPABILITY\", \"cc\"],\n",
    "    }\n",
    "    \n",
    "    feats = {}\n",
    "    label = \"  total_memory (bytes)\"\n",
    "    feats[label] = total\n",
    "    if show:\n",
    "        print(f\"{label:40}: {total}\")\n",
    "    for label, keys in props.items():\n",
    "        val = None\n",
    "        for k in keys:\n",
    "            val = getattr(dev, k, None)\n",
    "            feats[k] = val\n",
    "            if val is not None:\n",
    "                break\n",
    "        if show:\n",
    "            print(f\"{label:40}: {val}\")\n",
    "    return feats\n",
    "\n",
    "_ = device_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu_sum =  1048576\n",
      "gpu_sum =  1048576\n"
     ]
    }
   ],
   "source": [
    "# parallel reduction with no divergence to remind\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "@cuda.jit\n",
    "def parallelReductionNoDiv(array, out):\n",
    "    id = cuda.threadIdx.x\n",
    "    idx = cuda.grid(1)\n",
    "    # boundary checks\n",
    "    if idx >= len(array):\n",
    "        return\n",
    "\n",
    "    stride = 1\n",
    "    base = cuda.blockDim.x * cuda.blockIdx.x\n",
    "    while stride < cuda.blockDim.x:\n",
    "        index = 2*stride *id\n",
    "        if index + stride < cuda.blockDim.x:\n",
    "            array[base + index] += array[base + index + stride]\n",
    "        stride *= 2\n",
    "        cuda.syncthreads()\n",
    "\n",
    "\n",
    "    if id == 0:\n",
    "        out[cuda.blockIdx.x] = array[base]\n",
    "\n",
    "n = 1024 * 1024\n",
    "threads = 256\n",
    "blocks = (n + threads - 1) // threads\n",
    "\n",
    "h_array = np.ones(n, dtype=np.int32)\n",
    "\n",
    "cpu_sum = h_array.sum()\n",
    "\n",
    "d_array = cuda.to_device(h_array)\n",
    "d_out = cuda.device_array(blocks, dtype=np.int32)\n",
    "\n",
    "parallelReductionNoDiv[blocks, threads](d_array, d_out)\n",
    "cuda.synchronize()\n",
    "\n",
    "h_out = d_out.copy_to_host()\n",
    "gpu_sum = h_out.sum()\n",
    "\n",
    "print('cpu_sum = ',  cpu_sum)\n",
    "print('gpu_sum = ',  gpu_sum)\n",
    "\n",
    "assert gpu_sum == cpu_sum\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHR7Zs3dNs1N"
   },
   "source": [
    "# ‚úÖ Parallel reduction with shared memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ÜòÔ∏è TODO..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block Reduction with Shared Memory (Numba CUDA)\n",
    "\n",
    "-   Implement a **block-level reduction** kernel in **Numba CUDA** using **shared memory (SMEM)**.\n",
    "\n",
    "    -   Input: 1D array `array`\n",
    "    -   Output: 1D array `out` with **one partial sum per block**\n",
    "    -   Each block loads its elements into shared memory, then reduces them to a single sum.\n",
    "\n",
    "<br> üîπ **Learning Objectives**\n",
    "\n",
    "-   Allocate and use **shared memory** in a CUDA kernel\n",
    "-   Use `cuda.grid(1)` to compute a **global index**\n",
    "-   Apply the **standard reduction loop**: $$\n",
    "    \\text{stride} = \\frac{\\text{blockDim.x}}{2}, \\frac{\\text{blockDim.x}}{4}, \\dots, 1\n",
    "    $$\n",
    "-   **Synchronize threads** with `cuda.syncthreads()`\n",
    "-   **Write** **one result** per block into `out`\n",
    "\n",
    "<br> üîπ **Thread tasks...**\n",
    "\n",
    "-   Each thread:\n",
    "\n",
    "    -   **Loads** one element into shared memory\n",
    "\n",
    "    -   **Performs reduction** in shared memory\n",
    "\n",
    "    -   Thread `tid == 0` **writes the result** for the block\n",
    "\n",
    "<br> üîπ **Allocate Shared Memory**\n",
    "\n",
    "-   Shared array must have **compile-time constant size**:\n",
    "\n",
    "```{python}\n",
    "SMEM_SIZE = 1024\n",
    "smem = cuda.shared.array(SMEM_SIZE, dtype=np.float32)\n",
    "```\n",
    "\n",
    "-   This creates one shared buffer per block\n",
    "\n",
    "- Nest steps:\n",
    "    - Load Data into Shared Memory\n",
    "    - Shared Memory Reduction Loop\n",
    "    - Write One Result per Block\n",
    "\n",
    "- Template...\n",
    "\n",
    "```{python}\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "TPB = 256\n",
    "SMEM_SIZE = TPB\n",
    "\n",
    "@cuda.jit\n",
    "def blockParReduceSMEM(array, out, n):\n",
    "    tid = cuda.threadIdx.x\n",
    "    i = cuda.grid(1)\n",
    "\n",
    "    smem = cuda.shared.array(SMEM_SIZE, dtype=np.float32)\n",
    "\n",
    "    # TODO: load (with bounds check + padding)\n",
    "    # TODO: cuda.syncthreads()\n",
    "\n",
    "    # TODO: reduction loop\n",
    "\n",
    "    # TODO: write out[blockIdx.x]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu sum =  1048576\n",
      "cpu sum =  1048576\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "TPB = 256\n",
    "SMEM_SIZE = TPB\n",
    "\n",
    "@cuda.jit\n",
    "def blockParReduceSMEM(array, out, n):\n",
    "    tid = cuda.threadIdx.x\n",
    "    i = cuda.grid(1)\n",
    "\n",
    "    if i >= n:\n",
    "        return\n",
    "\n",
    "    smem = cuda.shared.array(SMEM_SIZE, dtype=np.int32)\n",
    "\n",
    "    if tid < cuda.blockDim.x:\n",
    "        smem[tid] = array[i]\n",
    "    cuda.syncthreads()\n",
    "\n",
    "    stride = cuda.blockDim.x // 2\n",
    "    while stride > 0:\n",
    "        if tid < stride:\n",
    "            smem[tid] += smem[tid + stride]\n",
    "        cuda.syncthreads()\n",
    "        stride //= 2\n",
    "\n",
    "    if tid == 0:\n",
    "        out[cuda.blockIdx.x] = smem[stride // 2 + tid]\n",
    "n = 1024 * 1024\n",
    "threads = SMEM_SIZE\n",
    "blocks = (n + threads - 1) // threads\n",
    "\n",
    "h_array = np.ones(n, dtype=np.int32)\n",
    "d_array = cuda.to_device(h_array)\n",
    "d_out = cuda.device_array_like(h_array)\n",
    "\n",
    "cpu_sum = h_array.sum()\n",
    "\n",
    "blockParReduceSMEM[blocks, threads](d_array, d_out, n)\n",
    "cuda.synchronize()\n",
    "h_out = d_out.copy_to_host()\n",
    "gpu_sum = h_out.sum()\n",
    "\n",
    "print('gpu sum = ', gpu_sum)\n",
    "print('cpu sum = ', cpu_sum)\n",
    "\n",
    "assert gpu_sum == cpu_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXUIQkZLCTcG"
   },
   "source": [
    "# ‚úÖ Matrix multiplication with shared memory (smem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy sum time: 0.0756 seconds\n",
      "Kernel blockParReduceSMEM execution time: 0.6124 seconds\n",
      "speedup over numpy: 0.12343717606015878\n",
      "[[3200. 3200. 3200. ... 3200. 3200. 3200.]\n",
      " [3200. 3200. 3200. ... 3200. 3200. 3200.]\n",
      " [3200. 3200. 3200. ... 3200. 3200. 3200.]\n",
      " ...\n",
      " [3200. 3200. 3200. ... 3200. 3200. 3200.]\n",
      " [3200. 3200. 3200. ... 3200. 3200. 3200.]\n",
      " [3200. 3200. 3200. ... 3200. 3200. 3200.]]\n",
      "[[3200. 3200. 3200. ... 3200. 3200. 3200.]\n",
      " [3200. 3200. 3200. ... 3200. 3200. 3200.]\n",
      " [3200. 3200. 3200. ... 3200. 3200. 3200.]\n",
      " ...\n",
      " [3200. 3200. 3200. ... 3200. 3200. 3200.]\n",
      " [3200. 3200. 3200. ... 3200. 3200. 3200.]\n",
      " [3200. 3200. 3200. ... 3200. 3200. 3200.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba import cuda,  float32\n",
    "import time\n",
    "\n",
    "@cuda.jit\n",
    "def matMul(A, B, C):\n",
    "    \"\"\"Perform square matrix multiplication of C = A * B.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : 2D array\n",
    "        Input matrix A\n",
    "    B : 2D array\n",
    "        Input matrix B\n",
    "    C : 2D array\n",
    "        Output matrix C\n",
    "    \"\"\"\n",
    "\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < C.shape[0] and j < C.shape[1]:\n",
    "        tmp = 0.0\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[i, k] * B[k, j]\n",
    "        C[i, j] = tmp\n",
    "\n",
    "#  Matrix sizes:\n",
    "#     A: (M x P) float32\n",
    "#     B: (P x N) float32\n",
    "#     C: (M x N) float32\n",
    "\n",
    "TPB = 16  # Threads per block\n",
    "N = TPB * 100  # Number of rows\n",
    "M = TPB * 100  # Number of columns\n",
    "P = TPB * 100  # Inner dimension\n",
    "\n",
    "# Initialize matrices\n",
    "A = np.ones((N,P), dtype=np.float32)   # A matrix \n",
    "B = 2* np.ones((P,M), dtype=np.float32)   # B matrix \n",
    "C = np.zeros((N, M), dtype=np.float32)  # Output matrix\n",
    "\n",
    "# verify numpy sum time\n",
    "tic = time.time()\n",
    "C_cpu = A @ B\n",
    "toc = time.time()\n",
    "print(f\"Numpy sum time: {toc - tic:.4f} seconds\")\n",
    "\n",
    "# GPU setup\n",
    "threads = (TPB, TPB)\n",
    "blocks = ((N + (threads[0] - 1)) // threads[0], (M + (threads[1] - 1)) // threads[1])\n",
    "d_A = cuda.to_device(A)\n",
    "d_B = cuda.to_device(B)\n",
    "d_C = cuda.device_array((N, M), dtype=A.dtype)\n",
    "\n",
    "# launch kernels and time\n",
    "t0 = time.perf_counter()\n",
    "matMul[blocks, threads](d_A, d_B, d_C)\n",
    "cuda.synchronize()\n",
    "t1 = time.perf_counter()\n",
    "print(f\"Kernel blockParReduceSMEM execution time: {t1 - t0:.4f} seconds\")\n",
    "print(\"speedup over numpy:\", (toc - tic) / (t1 - t0))\n",
    "\n",
    "# Final reduction on CPU\n",
    "C = d_C.copy_to_host()  # Final reduction in CPU\n",
    "\n",
    "# Verify correctness\n",
    "print(C)\n",
    "print(C_cpu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ÜòÔ∏è TODO..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> üîπ **Problem Definition**\n",
    "\n",
    "-   Given: $A$ of shape $(N, P)$, $B$ of shape $(P, M)$\n",
    "\n",
    "-   Compute: \n",
    "$$\n",
    "      C = A \\cdot B \\quad \\text{of shape } (N, M) \n",
    "$$\n",
    "\n",
    "-   Elementwise: \n",
    "    $$\n",
    "      C[y, x] = \\sum_{k=0}^{P-1} A[y, k] \\cdot B[k, x]\n",
    "    $$\n",
    "\n",
    "-   using **TPB√óTPB tiles** loaded into shared memory\n",
    "\n",
    "<br> üîπ **Learning Objectives**\n",
    "\n",
    "-   Launch a kernel with a **2D grid** and **2D blocks**\n",
    "-   Use `cuda.shared.array()` to create **shared-memory tiles**\n",
    "-   Implement a tiled dot product using **sweep over tiles**\n",
    "-   Use `cuda.syncthreads()` correctly\n",
    "-   Validate GPU results against NumPy (`A @ B`)\n",
    "-   Measure runtime and compute speedup\n",
    "\n",
    "<br> üîπ **CUDA Mapping**\n",
    "\n",
    "- Each thread computes one element of C:\n",
    "    - Thread global coordinates:\n",
    "    ```python\n",
    "    x, y = cuda.grid(2)\n",
    "    ```\n",
    "- Thread local coordinates in the block:\n",
    "\n",
    "```python\n",
    "tx = cuda.threadIdx.x\n",
    "ty = cuda.threadIdx.y\n",
    "```\n",
    "\n",
    "- So thread `(tx, ty)` in block `(bx, by)` computes `C[y, x]`\n",
    "\n",
    "<br> üîπ **Tiling Strategy (High Level)**\n",
    "\n",
    "- Instead of reading the full row/column from global memory, we:\n",
    "\t1.\tLoad a tile of A into shared memory sA\n",
    "\t2.\tLoad a tile of B into shared memory sB\n",
    "\t3.\tMultiply-accumulate inside the tile\n",
    "\t4.\tRepeat for all tiles along the inner dimension\n",
    "\n",
    "- This reduces global memory traffic\n",
    "\n",
    "<br> üîπ **Exercise Tasks**\n",
    "\n",
    "- Your tasks:\n",
    "\t1.\tCreate shared-memory tiles sA and sB\n",
    "\t2.\tCompute global coordinates (x, y) using cuda.grid(2)\n",
    "\t3.\tLoop over tiles along the inner dimension\n",
    "\t4.\tLoad tiles from A and B into shared memory\n",
    "\t5.\tSynchronize threads\n",
    "\t6.\tCompute partial dot product using shared memory\n",
    "\t7.\tSynchronize again before loading next tile\n",
    "\t8.\tStore final result in C[y, x]\n",
    "\n",
    "\n",
    "\n",
    "```{python}\n",
    "import numpy as np\n",
    "from numba import cuda, float32\n",
    "\n",
    "TPB = 16\n",
    "\n",
    "@cuda.jit\n",
    "def matMul_SMEM(A, B, C):\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "\n",
    "    # TODO: compute how many tiles along the inner dimension\n",
    "    # tiles = ...\n",
    "\n",
    "    tmp = float32(0.0)\n",
    "\n",
    "    # TODO: loop over tiles\n",
    "    # for i in range(tiles):\n",
    "\n",
    "        # TODO: load sA[ty, tx] from A (with bounds)\n",
    "        # TODO: load sB[ty, tx] from B (with bounds)\n",
    "\n",
    "        # TODO: synchronize\n",
    "        # cuda.syncthreads()\n",
    "\n",
    "        # TODO: compute partial dot product over j\n",
    "        # for j in range(TPB):\n",
    "        #     tmp += sA[ty, j] * sB[j, tx]\n",
    "\n",
    "        # TODO: synchronize before next tile\n",
    "        # cuda.syncthreads()\n",
    "\n",
    "    # TODO: store result in C (with bounds)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy sum time: 0.0819 seconds\n",
      "Kernel blockParReduceSMEM execution time: 0.5727 seconds\n",
      "speedup over numpy: 0.14296237670716028\n",
      "[[4.3649726e+12 4.3649755e+12 4.3649773e+12 ... 4.3690550e+12\n",
      "  4.3690576e+12 4.3690612e+12]\n",
      " [1.0914490e+13 1.0914497e+13 1.0914502e+13 ... 1.0926735e+13\n",
      "  1.0926744e+13 1.0926750e+13]\n",
      " [1.7463985e+13 1.7463999e+13 1.7464014e+13 ... 1.7484421e+13\n",
      "  1.7484434e+13 1.7484453e+13]\n",
      " ...\n",
      " [1.0463924e+16 1.0463932e+16 1.0463939e+16 ... 1.0476988e+16\n",
      "  1.0476995e+16 1.0477005e+16]\n",
      " [1.0470477e+16 1.0470483e+16 1.0470490e+16 ... 1.0483552e+16\n",
      "  1.0483558e+16 1.0483566e+16]\n",
      " [1.0477021e+16 1.0477028e+16 1.0477039e+16 ... 1.0490100e+16\n",
      "  1.0490108e+16 1.0490117e+16]]\n",
      "[[4.3649723e+12 4.3649744e+12 4.3649765e+12 ... 4.3690581e+12\n",
      "  4.3690602e+12 4.3690633e+12]\n",
      " [1.0914477e+13 1.0914483e+13 1.0914489e+13 ... 1.0926737e+13\n",
      "  1.0926744e+13 1.0926751e+13]\n",
      " [1.7463985e+13 1.7463996e+13 1.7464006e+13 ... 1.7484420e+13\n",
      "  1.7484432e+13 1.7484447e+13]\n",
      " ...\n",
      " [1.0463922e+16 1.0463930e+16 1.0463937e+16 ... 1.0476986e+16\n",
      "  1.0476994e+16 1.0477001e+16]\n",
      " [1.0470469e+16 1.0470477e+16 1.0470485e+16 ... 1.0483542e+16\n",
      "  1.0483549e+16 1.0483556e+16]\n",
      " [1.0477023e+16 1.0477030e+16 1.0477039e+16 ... 1.0490101e+16\n",
      "  1.0490109e+16 1.0490117e+16]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba import cuda, float32\n",
    "import time\n",
    "\n",
    "TPB = 16\n",
    "\n",
    "@cuda.jit\n",
    "def matMul_SMEM(A, B, C):\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "\n",
    "    # TODO: compute how many tiles along the inner dimension\n",
    "    tiles = P // TPB\n",
    "\n",
    "    tmp = float32(0.0)\n",
    "\n",
    "    for i in range(tiles):\n",
    "\n",
    "        if y < A.shape[0] and x < B.shape[1]:\n",
    "            sA[ty, tx] = A[y, i*TPB + tx]\n",
    "            sB[ty, tx] = B[i*TPB + ty, x] \n",
    "\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[ty, j] * sB[j, tx]\n",
    "\n",
    "        cuda.syncthreads()\n",
    "    if x < C.shape[0] and y < C.shape[1]:\n",
    "        C[y, x] = tmp\n",
    "\n",
    "TPB = 16  # Threads per block\n",
    "N = TPB * 100  # Number of rows\n",
    "M = TPB * 100  # Number of columns\n",
    "P = TPB * 100  # Inner dimension\n",
    "\n",
    "# Initialize matrices\n",
    "A = np.arange(stop=N * P, dtype=np.float32).reshape(N,P)   # A matrix \n",
    "B = 2* np.arange(stop=P*M, dtype=np.float32).reshape(P, M)   # B matrix \n",
    "# A = np.zeros((N,P), dtype=np.float32)\n",
    "# B = np.zeros((P,M), dtype=np.float32)\n",
    "# A[3,5] = 1\n",
    "# B[5,7] = 1\n",
    "\n",
    "C = np.zeros((N, M), dtype=np.float32)  # Output matrix\n",
    "\n",
    "# verify numpy sum time\n",
    "tic = time.time()\n",
    "C_cpu = A @ B\n",
    "toc = time.time()\n",
    "print(f\"Numpy sum time: {toc - tic:.4f} seconds\")\n",
    "\n",
    "# GPU setup\n",
    "threads = (TPB, TPB)\n",
    "blocks = ((N + (threads[0] - 1)) // threads[0], (M + (threads[1] - 1)) // threads[1])\n",
    "d_A = cuda.to_device(A)\n",
    "d_B = cuda.to_device(B)\n",
    "d_C = cuda.device_array((N, M), dtype=A.dtype)\n",
    "\n",
    "# launch kernels and time\n",
    "t0 = time.perf_counter()\n",
    "matMul_SMEM[blocks, threads](d_A, d_B, d_C)\n",
    "cuda.synchronize()\n",
    "t1 = time.perf_counter()\n",
    "print(f\"Kernel blockParReduceSMEM execution time: {t1 - t0:.4f} seconds\")\n",
    "print(\"speedup over numpy:\", (toc - tic) / (t1 - t0))\n",
    "\n",
    "# Final reduction on CPU\n",
    "C = d_C.copy_to_host()  # Final reduction in CPU\n",
    "\n",
    "# Verify correctness\n",
    "print(C)\n",
    "print(C_cpu)\n",
    "assert np.allclose(C, C_cpu, rtol=1e-5, atol=1e-6)\n",
    "# assert C_cpu[3, 7] == 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOFMQZAkjlLW"
   },
   "source": [
    "# ‚úÖ Convolution with smem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AT_wFzgkw-5W"
   },
   "source": [
    "## ‚ÜòÔ∏è TODO..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Host code that:\n",
    "\n",
    "  - allocates input `data`, `mask`, and output arrays\n",
    "  - computes a reference result with `np.convolve(..., mode=\"same\")`\n",
    "  - runs the shared-memory kernel\n",
    "  - runs the basic kernel\n",
    "  - prints timings and speed-ups\n",
    "  - checks maximum absolute error between host and device results\n",
    "\n",
    "You **do not need** to type all code from scratch ‚Äì focus on **reading & modifying**.\n",
    "\n",
    "<br> üîπ  **Understand the Parameters**\n",
    "\n",
    "- Inspect the parameter definitions:\n",
    "\n",
    "```python\n",
    "BLOCK_SIZE   = 1024\n",
    "MASK_RADIUS  = 100\n",
    "MASK_SIZE    = 2 * MASK_RADIUS + 1\n",
    "TILE_SIZE    = BLOCK_SIZE + MASK_SIZE - 1  # shared tile length per block\n",
    "\n",
    "n = 1024 * 1024 * 1024\n",
    "```\n",
    "\n",
    "\n",
    "<br> üîπ  **CPU Reference Convolution**\n",
    "\n",
    "- The host code computes:\n",
    "\n",
    "```python\n",
    "h_ref = np.convolve(data, mask, mode='same')\n",
    "```\n",
    "\n",
    "<br> üîπ  **Basic Kernel `conv1d_basic`**\n",
    "\n",
    "Skeleton:\n",
    "\n",
    "```python\n",
    "@cuda.jit\n",
    "def conv1d_basic(result, data, mask):\n",
    "    i = cuda.grid(1)\n",
    "    if i >= len(data):\n",
    "        return\n",
    "\n",
    "    mask_size = mask.shape[0]\n",
    "    radius = mask_size // 2\n",
    "\n",
    "    offset = i - radius\n",
    "    start = 0 if offset >= 0 else -offset\n",
    "    end = mask_size if (offset + mask_size) <= n else (n - offset)\n",
    "\n",
    "    acc = 0.0\n",
    "    for j in range(start, end):\n",
    "        acc += data[offset + j] * mask[j]\n",
    "\n",
    "    result[i] = acc\n",
    "```\n",
    "\n",
    "<br> üîπ  **Shared-Memory Kernel `conv1d_shared`**\n",
    "\n",
    "- Each block loads a **tile** into shared memory:\n",
    "  - left halo (MASK_RADIUS elements)\n",
    "  - center (block size elements)\n",
    "  - right halo (MASK_RADIUS elements)\n",
    "- Threads then read from shared memory instead of global memory\n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. Identify where **left halo**, **center**, and **right halo** are loaded in `conv1d_shared`.\n",
    "2. Explain why the code calls `cuda.syncthreads()` before performing the the convolution.\n",
    "3. Compare memory access patterns:\n",
    "   - basic kernel: global memory\n",
    "   - shared kernel: global ‚Üí shared ‚Üí reused\n",
    "\n",
    "<br> üîπ  **Launch Configuration**\n",
    "\n",
    "- The device launch configuration in the host code is:\n",
    "\n",
    "```python\n",
    "threads = BLOCK_SIZE\n",
    "blocks = (n + BLOCK_SIZE - 1) // BLOCK_SIZE\n",
    "conv1d_shared[blocks, threads](d_result, d_data, d_mask)\n",
    "```\n",
    "\n",
    "\n",
    "<br> üîπ **Timing & Speedup**\n",
    "\n",
    "At the end, the script prints:\n",
    "\n",
    "- `t_host` (CPU time)\n",
    "- `t_dev_shared` (GPU shared-memory mode)\n",
    "- `t_dev_basic` (GPU basic kernel)\n",
    "- Speedups: `host / shared`, `basic / shared`\n",
    "- Maximum absolute errors vs reference\n",
    "\n",
    "<br> üîπ  **Experiment: Different Masks**\n",
    "\n",
    "Currently, `mask` is:\n",
    "\n",
    "```python\n",
    "mask = np.ones(MASK_SIZE, dtype=np.float32)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      10       22       38 ... 19922887 16777178 12582890]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "BLOCK_SIZE = 1024\n",
    "MASK_RADIUS = 100\n",
    "MASK_SIZE = 2 * MASK_RADIUS + 1\n",
    "TILE_SIZE = BLOCK_SIZE + MASK_SIZE - 1 # shared tile length per block\n",
    "n = 1024 * 1024\n",
    "@cuda.jit\n",
    "def conv1d_basic(result, data, mask):\n",
    "    i = cuda.grid(1)\n",
    "    if i >= len(data):\n",
    "        return\n",
    "\n",
    "    mask_size = mask.shape[0]\n",
    "    radius = mask_size // 2\n",
    "\n",
    "    offset = i - radius\n",
    "    start = 0 if offset >= 0 else -offset\n",
    "    end = mask_size if (offset + mask_size) <= n else (n - offset)\n",
    "\n",
    "    acc = 0.0\n",
    "    for j in range(start, end):\n",
    "        acc += data[offset + j] * mask[j]\n",
    "\n",
    "    result[i] = acc\n",
    "\n",
    "@cuda.jit\n",
    "def conv1d_shared(result, data, mask):\n",
    "    smem = cuda.shared.array(TILE_SIZE, dtype=np.int32)\n",
    "    tid = cuda.threadIdx.x\n",
    "    idx = cuda.grid(1)\n",
    "\n",
    "    mask_size = mask.shape[0]\n",
    "    radius = mask_size // 2\n",
    "\n",
    "    left = cuda.blockDim.x * cuda.blockIdx.x - radius\n",
    "    right = (cuda.blockIdx.x + 1)*cuda.blockDim.x\n",
    "    \n",
    "    if tid < radius:\n",
    "        smem[tid] = 0 if left < 0 else data[left + tid]\n",
    "    elif tid >= cuda.blockDim.x - radius:\n",
    "        smem[tid + mask_size - 1] = 0 if right >= n else data[right + tid - cuda.blockDim.x + radius]\n",
    "\n",
    "    smem[tid + radius] = data[idx]\n",
    "    cuda.syncthreads()\n",
    "\n",
    "    acc = 0.0\n",
    "    for i in range(-radius, radius+1):\n",
    "        acc += smem[tid + radius + i] * mask[i + radius]\n",
    "\n",
    "    result[idx] = acc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "h_data = np.arange(stop=n, dtype=np.int32)\n",
    "h_mask = np.array([3, 4, 5, 4, 3])\n",
    "d_data = cuda.to_device(h_data)\n",
    "\n",
    "d_mask = cuda.to_device(h_mask)\n",
    "d_result = cuda.device_array_like(d_data)\n",
    "\n",
    "blocks = (n + BLOCK_SIZE -1) // BLOCK_SIZE\n",
    "conv1d_shared[blocks, BLOCK_SIZE](d_result, d_data, d_mask)\n",
    "cuda.synchronize()\n",
    "\n",
    "result = d_result.copy_to_host()\n",
    "print(result)\n",
    "assert np.array_equal(result, np.convolve(h_data, h_mask, mode='same'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "NO_C5o9-xRF_",
    "sQDPOWMUQJN8",
    "OHR7Zs3dNs1N",
    "vXUIQkZLCTcG",
    "SOFMQZAkjlLW"
   ],
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
