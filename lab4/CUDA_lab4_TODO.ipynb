{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZYqN0UwVLC_"
   },
   "source": [
    "---\n",
    "# **LAB 4 - CUDA memories**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ñ∂Ô∏è CUDA tools..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 27 15:00:29 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   32C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n",
      "0.60.0\n",
      "Found 1 CUDA devices\n",
      "id 0             b'Tesla T4'                              [SUPPORTED]\n",
      "                      Compute Capability: 7.5\n",
      "                           PCI Device ID: 4\n",
      "                              PCI Bus ID: 0\n",
      "                                    UUID: GPU-08256c08-312d-14b3-b5c0-5a02cbb6708e\n",
      "                                Watchdog: Disabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "Summary:\n",
      "\t1/1 devices are supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "from numba import cuda\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(np.__version__)\n",
    "print(numba.__version__)\n",
    "\n",
    "cuda.detect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Numba deprecation and performance warnings\n",
    "from numba.core.errors import NumbaDeprecationWarning, NumbaPerformanceWarning\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=NumbaPerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils for compiling and running Numba CUDA code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device object repr: <CUDA device 0 'b'Tesla T4''>\n",
      "Device name:           b'Tesla T4'\n",
      "Compute capability:    (7, 5)\n",
      "  total_memory (bytes)                  : 15828320256\n",
      "  multi_processor_(SM)_count            : 40\n",
      "  max_threads_per_block                 : 1024\n",
      "  max_block_dim_x                       : 1024\n",
      "  max_block_dim_y                       : 1024\n",
      "  max_block_dim_z                       : 64\n",
      "  max_grid_dim_x                        : 2147483647\n",
      "  max_grid_dim_y                        : 65535\n",
      "  max_grid_dim_z                        : 65535\n",
      "  max_shared_memory_per_block (bytes)   : 49152\n",
      "  max_shared_memory_per_SM (bytes)      : 65536\n",
      "  warp_size                             : 32\n",
      "  compute_capability                    : None\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "\n",
    "def mem_snapshot(print=True):\n",
    "    free, total = cuda.current_context().get_memory_info()\n",
    "    used = total - free\n",
    "    if print:\n",
    "        # print GPU memory info\n",
    "        print(\"\\nMemory occupancy:\")\n",
    "        print(f\"    GPU total: {total/1024**3:.3f} GB\")\n",
    "        print(f\"    GPU free : {free/1024**3:.3f} GB\")\n",
    "        print(f\"    GPU used : {used/1024**3:.3f} GB\")\n",
    "    else:\n",
    "        return used, free, total\n",
    "\n",
    "# Quick device spec report (Numba)\n",
    "def device_info(show=True):\n",
    "    dev = cuda.get_current_device()   # raises if no CUDA device\n",
    "    _, total = cuda.current_context().get_memory_info() \n",
    "    \n",
    "    if show:\n",
    "        print(\"Device object repr:\", dev)\n",
    "        print(\"Device name:          \", getattr(dev, \"name\", \"<unknown>\"))\n",
    "        print(\"Compute capability:   \", getattr(dev, \"compute_capability\", \"<unknown>\"))\n",
    "\n",
    "    # Common numeric properties (use getattr to avoid attribute errors)\n",
    "    props = {\n",
    "        \"  multi_processor_(SM)_count\": [\"MULTIPROCESSOR_COUNT\"],\n",
    "        \"  max_threads_per_block\": [\"MAX_THREADS_PER_BLOCK\"],\n",
    "        \"  max_block_dim_x\":       [\"MAX_BLOCK_DIM_X\"],\n",
    "        \"  max_block_dim_y\":       [\"MAX_BLOCK_DIM_Y\"],\n",
    "        \"  max_block_dim_z\":       [\"MAX_BLOCK_DIM_Z\"],\n",
    "        \"  max_grid_dim_x\":        [\"MAX_GRID_DIM_X\"],\n",
    "        \"  max_grid_dim_y\":        [\"MAX_GRID_DIM_Y\"],\n",
    "        \"  max_grid_dim_z\":        [\"MAX_GRID_DIM_Z\"],\n",
    "        \"  max_shared_memory_per_block (bytes)\": [\"MAX_SHARED_MEMORY_PER_BLOCK\"],\n",
    "        \"  max_shared_memory_per_SM (bytes)\": [\"MAX_SHARED_MEMORY_PER_MULTIPROCESSOR\"],\n",
    "        \"  warp_size\":             [\"WARP_SIZE\"],\n",
    "        \"  compute_capability\":    [\"COMPUTE_CAPABILITY\", \"cc\"],\n",
    "    }\n",
    "    \n",
    "    feats = {}\n",
    "    label = \"  total_memory (bytes)\"\n",
    "    feats[label] = total\n",
    "    if show:\n",
    "        print(f\"{label:40}: {total}\")\n",
    "    for label, keys in props.items():\n",
    "        val = None\n",
    "        for k in keys:\n",
    "            val = getattr(dev, k, None)\n",
    "            feats[k] = val\n",
    "            if val is not None:\n",
    "                break\n",
    "        if show:\n",
    "            print(f\"{label:40}: {val}\")\n",
    "    return feats\n",
    "\n",
    "_ = device_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHR7Zs3dNs1N"
   },
   "source": [
    "# ‚úÖ Parallel reduction with shared memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ÜòÔ∏è TODO..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block Reduction with Shared Memory (Numba CUDA)\n",
    "\n",
    "-   Implement a **block-level reduction** kernel in **Numba CUDA** using **shared memory (SMEM)**.\n",
    "\n",
    "    -   Input: 1D array `array`\n",
    "    -   Output: 1D array `out` with **one partial sum per block**\n",
    "    -   Each block loads its elements into shared memory, then reduces them to a single sum.\n",
    "\n",
    "<br> üîπ **Learning Objectives**\n",
    "\n",
    "-   Allocate and use **shared memory** in a CUDA kernel\n",
    "-   Use `cuda.grid(1)` to compute a **global index**\n",
    "-   Apply the **standard reduction loop**: $$\n",
    "    \\text{stride} = \\frac{\\text{blockDim.x}}{2}, \\frac{\\text{blockDim.x}}{4}, \\dots, 1\n",
    "    $$\n",
    "-   **Synchronize threads** with `cuda.syncthreads()`\n",
    "-   **Write** **one result** per block into `out`\n",
    "\n",
    "<br> üîπ **Thread tasks...**\n",
    "\n",
    "-   Each thread:\n",
    "\n",
    "    -   **Loads** one element into shared memory\n",
    "\n",
    "    -   **Performs reduction** in shared memory\n",
    "\n",
    "    -   Thread `tid == 0` **writes the result** for the block\n",
    "\n",
    "<br> üîπ **Allocate Shared Memory**\n",
    "\n",
    "-   Shared array must have **compile-time constant size**:\n",
    "\n",
    "```{python}\n",
    "SMEM_SIZE = 1024\n",
    "smem = cuda.shared.array(SMEM_SIZE, dtype=np.float32)\n",
    "```\n",
    "\n",
    "-   This creates one shared buffer per block\n",
    "\n",
    "- Nest steps:\n",
    "    - Load Data into Shared Memory\n",
    "    - Shared Memory Reduction Loop\n",
    "    - Write One Result per Block\n",
    "\n",
    "- Template...\n",
    "\n",
    "```{python}\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "TPB = 256\n",
    "SMEM_SIZE = TPB\n",
    "\n",
    "@cuda.jit\n",
    "def blockParReduceSMEM(array, out, n):\n",
    "    tid = cuda.threadIdx.x\n",
    "    i = cuda.grid(1)\n",
    "\n",
    "    smem = cuda.shared.array(SMEM_SIZE, dtype=np.float32)\n",
    "\n",
    "    # TODO: load (with bounds check + padding)\n",
    "    # TODO: cuda.syncthreads()\n",
    "\n",
    "    # TODO: reduction loop\n",
    "\n",
    "    # TODO: write out[blockIdx.x]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:numba.cuda.cudadrv.driver:Call to cuMemAlloc results in CUDA_ERROR_ILLEGAL_ADDRESS\n"
     ]
    },
    {
     "ename": "CudaAPIError",
     "evalue": "[700] Call to cuMemAlloc results in CUDA_ERROR_ILLEGAL_ADDRESS",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCudaAPIError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3510821167.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# prepare data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0ma_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mb_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumBlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/cudadrv/devices.py\u001b[0m in \u001b[0;36m_require_cuda_context\u001b[0;34m(*args, **kws)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_require_cuda_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_runtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_require_cuda_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/api.py\u001b[0m in \u001b[0;36mto_device\u001b[0;34m(obj, stream, copy, to)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \"\"\"\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         to, new = devicearray.auto_device(\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_explicit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/cudadrv/devicearray.py\u001b[0m in \u001b[0;36mauto_device\u001b[0;34m(obj, stream, copy, user_explicit)\u001b[0m\n\u001b[1;32m    921\u001b[0m             )\n\u001b[1;32m    922\u001b[0m             \u001b[0msentry_contiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m             \u001b[0mdevobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_array_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             if (\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/cudadrv/devicearray.py\u001b[0m in \u001b[0;36mfrom_array_like\u001b[0;34m(ary, stream, gpu_data)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfrom_array_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;34m\"Create a DeviceNDArray object that is like ary.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m     return DeviceNDArray(\n\u001b[0m\u001b[1;32m    843\u001b[0m         \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpu_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/cudadrv/devicearray.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, shape, strides, dtype, stream, gpu_data)\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 )\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mgpu_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemalloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malloc_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malloc_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_driver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_memory_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36mmemalloc\u001b[0;34m(self, bytesize)\u001b[0m\n\u001b[1;32m   1448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmemalloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytesize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1450\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemalloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytesize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1452\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmemallocmanaged\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytesize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattach_global\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36mmemalloc\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1114\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuMemAlloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0mptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attempt_allocation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallocator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m             \u001b[0malloc_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36m_attempt_allocation\u001b[0;34m(self, allocator)\u001b[0m\n\u001b[1;32m    905\u001b[0m         \"\"\"\n\u001b[1;32m    906\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mallocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCudaAPIError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0;31m# is out-of-memory?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36mallocator\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mallocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuMemAlloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0mptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attempt_allocation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallocator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36msafe_cuda_api_call\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0msafe_cuda_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"call driver api: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_cuda_python_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCUDA_LOG_API_ARGS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36m_check_cuda_python_error\u001b[0;34m(self, fname, returned)\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretcode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbinding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCUresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCUDA_ERROR_NOT_INITIALIZED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_fork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCudaAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCudaAPIError\u001b[0m: [700] Call to cuMemAlloc results in CUDA_ERROR_ILLEGAL_ADDRESS"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "# Parellel reduction with no divergence\n",
    "@cuda.jit\n",
    "def blockParReduceSMEM(in_arr, out_arr):\n",
    "    tid = cuda.threadIdx.x\n",
    "    idx = cuda.grid(1) # global index\n",
    "    n = len(in_arr)\n",
    "    stride = 1\n",
    "    base = cuda.blockIdx.x * cuda.blockDim.x\n",
    "\n",
    "    if idx >= n:\n",
    "            return\n",
    "\n",
    "    while stride < cuda.blockDim.x:\n",
    "        index = 2 * stride * tid\n",
    "        if index < cuda.blockDim.x:\n",
    "            in_arr[base + index] += in_arr[base + index + stride]\n",
    "        stride *= 2\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    if tid == 0:\n",
    "        out_arr[cuda.blockIdx.x] = in_arr[base]\n",
    "\n",
    "# ----------------------------\n",
    "# host-side usage\n",
    "# ----------------------------\n",
    "blockSize = 1024;               # block dim 1D\n",
    "numBlock = 1024*1024          # grid dim 1D\n",
    "n = blockSize * numBlock;       # array dim\n",
    "\n",
    "# prepare data\n",
    "a = np.ones(n, dtype=np.int32)\n",
    "a_d = cuda.to_device(a)\n",
    "b_d = cuda.device_array(numBlock, dtype=np.int32)\n",
    "\n",
    "# numpy sum time\n",
    "tic = time.time()\n",
    "s_cpu = a.sum()\n",
    "toc = time.time()\n",
    "print(f\"Numpy sum time: {toc - tic:.4f} seconds\")\n",
    "\n",
    "# launch kernel\n",
    "t0 = time.perf_counter()\n",
    "blockParReduceSMEM[numBlock, blockSize](a_d, b_d)\n",
    "cuda.synchronize()\n",
    "t1 = time.perf_counter()\n",
    "print(f\"Kernel execution time: {t1 - t0:.4f} seconds\")\n",
    "print(\"speedup over numpy:\", (toc - tic) / (t1 - t0))\n",
    "\n",
    "# copy result back to host\n",
    "b = b_d.copy_to_host()\n",
    "s_gpu = b.sum()\n",
    "print('GPU sum = ', s_gpu, ' CPU sum = ', s_cpu)\n",
    "assert s_cpu == s_gpu, \"Error! Reduction result does not match!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXUIQkZLCTcG"
   },
   "source": [
    "# ‚úÖ Matrix multiplication with shared memory (smem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda,  float32\n",
    "import time\n",
    "\n",
    "@cuda.jit\n",
    "def matMul(A, B, C):\n",
    "    \"\"\"Perform square matrix multiplication of C = A * B.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : 2D array\n",
    "        Input matrix A\n",
    "    B : 2D array\n",
    "        Input matrix B\n",
    "    C : 2D array\n",
    "        Output matrix C\n",
    "    \"\"\"\n",
    "\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < C.shape[0] and j < C.shape[1]:\n",
    "        tmp = 0.0\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[i, k] * B[k, j]\n",
    "        C[i, j] = tmp\n",
    "\n",
    "#  Matrix sizes:\n",
    "#     A: (M x P) float32\n",
    "#     B: (P x N) float32\n",
    "#     C: (M x N) float32\n",
    "\n",
    "TPB = 16  # Threads per block\n",
    "N = TPB * 1000  # Number of rows\n",
    "M = TPB * 1000  # Number of columns\n",
    "P = TPB * 1000  # Inner dimension\n",
    "\n",
    "# Initialize matrices\n",
    "A = np.ones((N,P), dtype=np.float32)   # A matrix \n",
    "B = 2* np.ones((P,M), dtype=np.float32)   # B matrix \n",
    "C = np.zeros((N, M), dtype=np.float32)  # Output matrix\n",
    "\n",
    "# verify numpy sum time\n",
    "tic = time.time()\n",
    "C_cpu = A @ B\n",
    "toc = time.time()\n",
    "print(f\"Numpy sum time: {toc - tic:.4f} seconds\")\n",
    "\n",
    "# GPU setup\n",
    "threads = (TPB, TPB)\n",
    "blocks = ((N + (threads[0] - 1)) // threads[0], (M + (threads[1] - 1)) // threads[1])\n",
    "d_A = cuda.to_device(A)\n",
    "d_B = cuda.to_device(B)\n",
    "d_C = cuda.device_array((N, M), dtype=A.dtype)\n",
    "\n",
    "# launch kernels and time\n",
    "t0 = time.perf_counter()\n",
    "matMul[blocks, threads](d_A, d_B, d_C)\n",
    "cuda.synchronize()\n",
    "t1 = time.perf_counter()\n",
    "print(f\"Kernel blockParReduceSMEM execution time: {t1 - t0:.4f} seconds\")\n",
    "print(\"speedup over numpy:\", (toc - tic) / (t1 - t0))\n",
    "\n",
    "# Final reduction on CPU\n",
    "C = d_C.copy_to_host()  # Final reduction in CPU\n",
    "\n",
    "# Verify correctness\n",
    "print(C)\n",
    "print(C_cpu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ÜòÔ∏è TODO..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> üîπ **Problem Definition**\n",
    "\n",
    "-   Given: $A$ of shape $(N, P)$, $B$ of shape $(P, M)$\n",
    "\n",
    "-   Compute: \n",
    "$$\n",
    "      C = A \\cdot B \\quad \\text{of shape } (N, M) \n",
    "$$\n",
    "\n",
    "-   Elementwise: \n",
    "    $$\n",
    "      C[y, x] = \\sum_{k=0}^{P-1} A[y, k] \\cdot B[k, x]\n",
    "    $$\n",
    "\n",
    "-   using **TPB√óTPB tiles** loaded into shared memory\n",
    "\n",
    "<br> üîπ **Learning Objectives**\n",
    "\n",
    "-   Launch a kernel with a **2D grid** and **2D blocks**\n",
    "-   Use `cuda.shared.array()` to create **shared-memory tiles**\n",
    "-   Implement a tiled dot product using **sweep over tiles**\n",
    "-   Use `cuda.syncthreads()` correctly\n",
    "-   Validate GPU results against NumPy (`A @ B`)\n",
    "-   Measure runtime and compute speedup\n",
    "\n",
    "<br> üîπ **CUDA Mapping**\n",
    "\n",
    "- Each thread computes one element of C:\n",
    "    - Thread global coordinates:\n",
    "    ```python\n",
    "    x, y = cuda.grid(2)\n",
    "    ```\n",
    "- Thread local coordinates in the block:\n",
    "\n",
    "```python\n",
    "tx = cuda.threadIdx.x\n",
    "ty = cuda.threadIdx.y\n",
    "```\n",
    "\n",
    "- So thread `(tx, ty)` in block `(bx, by)` computes `C[y, x]`\n",
    "\n",
    "<br> üîπ **Tiling Strategy (High Level)**\n",
    "\n",
    "- Instead of reading the full row/column from global memory, we:\n",
    "\t1.\tLoad a tile of A into shared memory sA\n",
    "\t2.\tLoad a tile of B into shared memory sB\n",
    "\t3.\tMultiply-accumulate inside the tile\n",
    "\t4.\tRepeat for all tiles along the inner dimension\n",
    "\n",
    "- This reduces global memory traffic\n",
    "\n",
    "<br> üîπ **Exercise Tasks**\n",
    "\n",
    "- Your tasks:\n",
    "\t1.\tCreate shared-memory tiles sA and sB\n",
    "\t2.\tCompute global coordinates (x, y) using cuda.grid(2)\n",
    "\t3.\tLoop over tiles along the inner dimension\n",
    "\t4.\tLoad tiles from A and B into shared memory\n",
    "\t5.\tSynchronize threads\n",
    "\t6.\tCompute partial dot product using shared memory\n",
    "\t7.\tSynchronize again before loading next tile\n",
    "\t8.\tStore final result in C[y, x]\n",
    "\n",
    "\n",
    "\n",
    "```{python}\n",
    "import numpy as np\n",
    "from numba import cuda, float32\n",
    "\n",
    "TPB = 16\n",
    "\n",
    "@cuda.jit\n",
    "def matMul_SMEM(A, B, C):\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "\n",
    "    # TODO: compute how many tiles along the inner dimension\n",
    "    # tiles = ...\n",
    "\n",
    "    tmp = float32(0.0)\n",
    "\n",
    "    # TODO: loop over tiles\n",
    "    # for i in range(tiles):\n",
    "\n",
    "        # TODO: load sA[ty, tx] from A (with bounds)\n",
    "        # TODO: load sB[ty, tx] from B (with bounds)\n",
    "\n",
    "        # TODO: synchronize\n",
    "        # cuda.syncthreads()\n",
    "\n",
    "        # TODO: compute partial dot product over j\n",
    "        # for j in range(TPB):\n",
    "        #     tmp += sA[ty, j] * sB[j, tx]\n",
    "\n",
    "        # TODO: synchronize before next tile\n",
    "        # cuda.syncthreads()\n",
    "\n",
    "    # TODO: store result in C (with bounds)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda, float32\n",
    "\n",
    "TPB = 16\n",
    "\n",
    "@cuda.jit\n",
    "def matMul_SMEM(A, B, C):\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "\n",
    "    # TODO: compute how many tiles along the inner dimension\n",
    "    # tiles = ...\n",
    "\n",
    "    tmp = float32(0.0)\n",
    "\n",
    "    # TODO: loop over tiles\n",
    "    # for i in range(tiles):\n",
    "\n",
    "        # TODO: load sA[ty, tx] from A (with bounds)\n",
    "        # TODO: load sB[ty, tx] from B (with bounds)\n",
    "\n",
    "        # TODO: synchronize\n",
    "        # cuda.syncthreads()\n",
    "\n",
    "        # TODO: compute partial dot product over j\n",
    "        # for j in range(TPB):\n",
    "        #     tmp += sA[ty, j] * sB[j, tx]\n",
    "\n",
    "        # TODO: synchronize before next tile\n",
    "        # cuda.syncthreads()\n",
    "\n",
    "    # TODO: store result in C (with bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOFMQZAkjlLW"
   },
   "source": [
    "# ‚úÖ Convolution with smem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AT_wFzgkw-5W"
   },
   "source": [
    "## ‚ÜòÔ∏è TODO..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Host code that:\n",
    "\n",
    "  - allocates input `data`, `mask`, and output arrays\n",
    "  - computes a reference result with `np.convolve(..., mode=\"same\")`\n",
    "  - runs the shared-memory kernel\n",
    "  - runs the basic kernel\n",
    "  - prints timings and speed-ups\n",
    "  - checks maximum absolute error between host and device results\n",
    "\n",
    "You **do not need** to type all code from scratch ‚Äì focus on **reading & modifying**.\n",
    "\n",
    "<br> üîπ  **Understand the Parameters**\n",
    "\n",
    "- Inspect the parameter definitions:\n",
    "\n",
    "```python\n",
    "BLOCK_SIZE   = 1024\n",
    "MASK_RADIUS  = 100\n",
    "MASK_SIZE    = 2 * MASK_RADIUS + 1\n",
    "TILE_SIZE    = BLOCK_SIZE + MASK_SIZE - 1  # shared tile length per block\n",
    "\n",
    "n = 1024 * 1024 * 1024\n",
    "```\n",
    "\n",
    "\n",
    "<br> üîπ  **CPU Reference Convolution**\n",
    "\n",
    "- The host code computes:\n",
    "\n",
    "```python\n",
    "h_ref = np.convolve(data, mask, mode='same')\n",
    "```\n",
    "\n",
    "<br> üîπ  **Basic Kernel `conv1d_basic`**\n",
    "\n",
    "Skeleton:\n",
    "\n",
    "```python\n",
    "@cuda.jit\n",
    "def conv1d_basic(result, data, mask):\n",
    "    i = cuda.grid(1)\n",
    "    if i >= len(data):\n",
    "        return\n",
    "\n",
    "    mask_size = mask.shape[0]\n",
    "    radius = mask_size // 2\n",
    "\n",
    "    offset = i - radius\n",
    "    start = 0 if offset >= 0 else -offset\n",
    "    end = mask_size if (offset + mask_size) <= n else (n - offset)\n",
    "\n",
    "    acc = 0.0\n",
    "    for j in range(start, end):\n",
    "        acc += data[offset + j] * mask[j]\n",
    "\n",
    "    result[i] = acc\n",
    "```\n",
    "\n",
    "<br> üîπ  **Shared-Memory Kernel `conv1d_shared`**\n",
    "\n",
    "- Each block loads a **tile** into shared memory:\n",
    "  - left halo (MASK_RADIUS elements)\n",
    "  - center (block size elements)\n",
    "  - right halo (MASK_RADIUS elements)\n",
    "- Threads then read from shared memory instead of global memory\n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. Identify where **left halo**, **center**, and **right halo** are loaded in `conv1d_shared`.\n",
    "2. Explain why the code calls `cuda.syncthreads()` before performing the the convolution.\n",
    "3. Compare memory access patterns:\n",
    "   - basic kernel: global memory\n",
    "   - shared kernel: global ‚Üí shared ‚Üí reused\n",
    "\n",
    "<br> üîπ  **Launch Configuration**\n",
    "\n",
    "- The device launch configuration in the host code is:\n",
    "\n",
    "```python\n",
    "threads = BLOCK_SIZE\n",
    "blocks = (n + BLOCK_SIZE - 1) // BLOCK_SIZE\n",
    "conv1d_shared[blocks, threads](d_result, d_data, d_mask)\n",
    "```\n",
    "\n",
    "\n",
    "<br> üîπ **Timing & Speedup**\n",
    "\n",
    "At the end, the script prints:\n",
    "\n",
    "- `t_host` (CPU time)\n",
    "- `t_dev_shared` (GPU shared-memory mode)\n",
    "- `t_dev_basic` (GPU basic kernel)\n",
    "- Speedups: `host / shared`, `basic / shared`\n",
    "- Maximum absolute errors vs reference\n",
    "\n",
    "<br> üîπ  **Experiment: Different Masks**\n",
    "\n",
    "Currently, `mask` is:\n",
    "\n",
    "```python\n",
    "mask = np.ones(MASK_SIZE, dtype=np.float32)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "NO_C5o9-xRF_",
    "sQDPOWMUQJN8",
    "OHR7Zs3dNs1N",
    "vXUIQkZLCTcG",
    "SOFMQZAkjlLW"
   ],
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
