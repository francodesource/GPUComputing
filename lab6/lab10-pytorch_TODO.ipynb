{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f48db13",
   "metadata": {},
   "source": [
    "---\n",
    "# **Lab10: PyTorch**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f07740a",
   "metadata": {},
   "source": [
    "# ‚ñ∂Ô∏è GPU tools..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "defbcb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb  4 16:08:38 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   45C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60144ded",
   "metadata": {},
   "source": [
    "# ‚úÖ Batch Normalization for RGB Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d02cb1",
   "metadata": {},
   "source": [
    "Define a tensor of shape $[B, 3, H, W]$ filled with random values to simulate a batch of RGB images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57d3a932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([8, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define batch parameters\n",
    "B, C, H, W = 8, 3, 64, 64\n",
    "images = torch.rand(B, C, H, W)\n",
    "\n",
    "print(\"Batch shape:\", images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa423af",
   "metadata": {},
   "source": [
    "## ‚ÜòÔ∏è TODO..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f685e3b9",
   "metadata": {},
   "source": [
    "**Steps to perform channel-wise batch normalization**:\n",
    "\n",
    "1. Compute the **mean per color channel** by averaging over the batch and spatial dimensions\n",
    "2. Compute the **standard deviation per color channel** over the same dimensions\n",
    "3. Inspect the computed statistics to ensure they correspond to the RGB channels\n",
    "    - print(\"Mean per channel:\", mean)\n",
    "    - print(\"Std per channel:\", std)\n",
    "\n",
    "\n",
    "4. Reshape the mean and standard deviation tensors to make them compatible with broadcasting\n",
    "    - Verify the reshaped dimensions:\n",
    "        - print(\"Reshaped mean:\", mean.shape)\n",
    "        - print(\"Reshaped std:\", std.shape)\n",
    "\n",
    "\n",
    "5. Apply channel-wise centering and normalization using broadcasting\n",
    "\n",
    "6. Verify the result by recomputing the mean and standard deviation of the normalized batch\n",
    "7. Check that the normalized images have approximately zero mean and unit variance per channel\n",
    "\n",
    "    - print(\"New mean per channel:\", new_mean)\n",
    "    - print(\"New std per channel:\", new_std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28be6140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean per channel:  tensor([0.9453, 1.8125, 0.7812])\n",
      "Std per channel:  tensor([1.8403e+08, 3.4465e+08, 1.6559e+08])\n",
      "Reshaped mean:  tensor([[[[0.9453]],\n",
      "\n",
      "         [[1.8125]],\n",
      "\n",
      "         [[0.7812]]]])\n",
      "Reshaped std:  tensor([[[[1.8403e+08]],\n",
      "\n",
      "         [[3.4465e+08]],\n",
      "\n",
      "         [[1.6559e+08]]]])\n",
      "New mean per channel:  tensor([4.9331e-09, 4.3656e-09, 4.1910e-09])\n",
      "New std per channel:  tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "mean = images.mean(dim=[0, 2, 3])\n",
    "std = images.std(dim=[0, 2, 3])\n",
    "\n",
    "print(\"Mean per channel: \", mean)\n",
    "print(\"Std per channel: \", std)\n",
    "# reshaping\n",
    "mean = mean.view(1, -1, 1, 1)\n",
    "std = std.view(1, -1, 1, 1)\n",
    "print(\"Reshaped mean: \", mean)\n",
    "print(\"Reshaped std: \", std)\n",
    "\n",
    "normalized_images = (images - mean) /  std\n",
    "new_mean = normalized_images.mean(dim=[0, 2, 3])\n",
    "new_std = normalized_images.std(dim=[0, 2, 3])\n",
    "print(\"New mean per channel: \", new_mean)\n",
    "print(\"New std per channel: \", new_std)\n",
    "\n",
    "import numpy as np\n",
    "# checks\n",
    "assert np.allclose(new_mean, 0.0, atol=1e-5) \n",
    "assert np.allclose(new_std, 1.0, atol=1e-5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748ad276",
   "metadata": {},
   "source": [
    "# ‚úÖ Mini multi-head attention block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6c521d",
   "metadata": {},
   "source": [
    "## ‚ÜòÔ∏è TODO..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5020765d",
   "metadata": {},
   "source": [
    "- Implement a **mini multi-head attention block** *from scratch* using only:\n",
    "    - tensor reshaping (`view`, `reshape`, `transpose`, `permute`)\n",
    "    - batched matrix multiplication (`matmul`)\n",
    "    - broadcasting\n",
    "\n",
    "\n",
    "<br> üîπ **Setup**\n",
    "\n",
    "- batch size $B = 4$\n",
    "- sequence length $T = 16$\n",
    "- embedding dimension $D = 64$\n",
    "- number of heads $H = 8$ (so head dimension $d = D/H = 8$)\n",
    "\n",
    "- Create:\n",
    "    - input tensor `x` with shape `(B, T, D)`  \n",
    "    - learnable projection matrices `Wq, Wk, Wv, Wo` with shapes `(D, D)`\n",
    "\n",
    "\n",
    "<br> üîπ **Tasks**\n",
    "\n",
    "\n",
    "- **1) Create the input**\n",
    "  - Generate `x = torch.randn(B, T, D, requires_grad=True)`\n",
    "<br>\n",
    "\n",
    "\n",
    "- **2) Compute Q, K, V**\n",
    "  - Compute:\n",
    "    - `Q = x @ Wq`\n",
    "    - `K = x @ Wk`\n",
    "    - `V = x @ Wv`\n",
    "  - Ensure each has shape `(B, T, D)`\n",
    "<br>\n",
    "\n",
    "- **3) Split into heads**\n",
    "  - Reshape and permute so that:\n",
    "    - `Qh, Kh, Vh` have shape `(B, H, T, d)`\n",
    "  - Hint: use `.view(B, T, H, d)` then `.transpose(1, 2)`\n",
    "<br>\n",
    "\n",
    "\n",
    "- **4) Compute attention scores**\n",
    "  - Compute scaled dot-product attention:\n",
    "    $$\n",
    "    S = \\frac{Q_h K_h^T}{\\sqrt{d}}\n",
    "    $$\n",
    "  - `S` must have shape `(B, H, T, T)`\n",
    "  - Use `torch.matmul(Qh, Kh.transpose(-2, -1))`\n",
    "<br>\n",
    "\n",
    "- **5) Softmax + weighted sum**\n",
    "  - Apply:\n",
    "    $$\n",
    "    A = \\mathrm{softmax}(S)\n",
    "    $$\n",
    "    $$\n",
    "    O_h = A V_h\n",
    "    $$\n",
    "  - `Oh` must have shape `(B, H, T, d)`\n",
    "<br>\n",
    "\n",
    "- **6) Merge heads**\n",
    "  - Convert `(B, H, T, d)` back to `(B, T, D)` using transpose + reshape.\n",
    "<br>\n",
    "\n",
    "- **7) Output projection**\n",
    "  - Compute final output:\n",
    "    - `y = out @ Wo`\n",
    "  - Shape must be `(B, T, D)`\n",
    "\n",
    "<br> üîπ What to Submit\n",
    "\n",
    "- A short printout of shapes at each step:\n",
    "  - `Qh.shape`, `S.shape`, `A.shape`, `Oh.shape`, `y.shape`\n",
    "- Confirmation that gradients are computed\n",
    "\n",
    "\n",
    "<br> üîπ **Expected Key Shapes**\n",
    "\n",
    "- `x`: `(B, T, D)`\n",
    "- `Qh, Kh, Vh`: `(B, H, T, d)`\n",
    "- `S`: `(B, H, T, T)`\n",
    "- `A`: `(B, H, T, T)`\n",
    "- `Oh`: `(B, H, T, d)`\n",
    "- `y`: `(B, T, D)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1e644d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# step 1\n",
    "B, T, D, H = 4, 16, 64, 8\n",
    "d = D // H\n",
    "\n",
    "\n",
    "\n",
    "Wq, Wk, Wv, Wo = [torch.randn(D, D) for _ in range(4)]\n",
    "x = torch.randn(B, T, D, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f217c6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2\n",
    "Q = x @ Wq\n",
    "K = x @ Wk\n",
    "V = x @ Wv\n",
    "\n",
    "for v in [Q, K, V]:\n",
    "    assert v.shape == (B, T, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b1815a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# step 3\n",
    "print(d)\n",
    "Qh = Q.view(B, T, H, d).transpose(1, 2)\n",
    "Kh = K.view(B, T, H, d).transpose(1, 2)\n",
    "Vh = V.view(B, T, H, d).transpose(1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e550e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4 \n",
    "import math\n",
    "S = (Qh @ Kh.transpose(-2, -1)) / math.sqrt(d)\n",
    "\n",
    "assert S.shape == (B, H, T, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17a18969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 5\n",
    "A = torch.softmax(S, dim=-1)\n",
    "Oh = A @ Vh\n",
    "assert Oh.shape == (B, H, T, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d167f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [4, 16] but got: [4, 64].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1397846964.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.reshape(B, T, D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mWo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Qh shape: {Qh.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [4, 16] but got: [4, 64]."
     ]
    }
   ],
   "source": [
    "out = Oh.transpose(1, 2).reshape(B, T, D)\n",
    "y = out @ Wo\n",
    "assert y.shape == (B, T, D)\n",
    "\n",
    "print(f'Qh shape: {Qh.shape}')\n",
    "print(f'S shape: {S.shape}')\n",
    "print(f'A shape: {A.shape}')\n",
    "print(f'Oh shape: {Oh.shape}')\n",
    "print(f'y shape: {y.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e4f64120",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1528074327.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mOh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tests\n",
    "assert x.shape == (B, T, D)\n",
    "for t in [Qh, Kh, Vh]:\n",
    "    assert t.shape == (B, H, T, d)\n",
    "assert S.shape == (B, H, T, T)\n",
    "assert A.shape == (B, H, T, T)\n",
    "assert Oh.shape == (B, H, T, d)\n",
    "assert y.shape == (B, T, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037b8930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
