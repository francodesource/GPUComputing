{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZYqN0UwVLC_"
   },
   "source": [
    "---\n",
    "# **LAB 5 - CUDA Streams**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ñ∂Ô∏è CUDA tools..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "from numba import cuda\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(np.__version__)\n",
    "print(numba.__version__)\n",
    "\n",
    "cuda.detect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Numba deprecation and performance warnings\n",
    "from numba.core.errors import NumbaDeprecationWarning, NumbaPerformanceWarning\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=NumbaPerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils for compiling and running Numba CUDA code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "def mem_snapshot(print=True):\n",
    "    free, total = cuda.current_context().get_memory_info()\n",
    "    used = total - free\n",
    "    if print:\n",
    "        # print GPU memory info\n",
    "        print(\"\\nMemory occupancy:\")\n",
    "        print(f\"    GPU total: {total/1024**3:.3f} GB\")\n",
    "        print(f\"    GPU free : {free/1024**3:.3f} GB\")\n",
    "        print(f\"    GPU used : {used/1024**3:.3f} GB\")\n",
    "    else:\n",
    "        return used, free, total\n",
    "\n",
    "# Quick device spec report (Numba)\n",
    "def device_info(show=True):\n",
    "    dev = cuda.get_current_device()   # raises if no CUDA device\n",
    "    _, total = cuda.current_context().get_memory_info() \n",
    "    \n",
    "    if show:\n",
    "        print(\"Device object repr:\", dev)\n",
    "        print(\"Device name:          \", getattr(dev, \"name\", \"<unknown>\"))\n",
    "        print(\"Compute capability:   \", getattr(dev, \"compute_capability\", \"<unknown>\"))\n",
    "\n",
    "    # Common numeric properties (use getattr to avoid attribute errors)\n",
    "    props = {\n",
    "        \"  multi_processor_(SM)_count\": [\"MULTIPROCESSOR_COUNT\"],\n",
    "        \"  max_threads_per_block\": [\"MAX_THREADS_PER_BLOCK\"],\n",
    "        \"  max_block_dim_x\":       [\"MAX_BLOCK_DIM_X\"],\n",
    "        \"  max_block_dim_y\":       [\"MAX_BLOCK_DIM_Y\"],\n",
    "        \"  max_block_dim_z\":       [\"MAX_BLOCK_DIM_Z\"],\n",
    "        \"  max_grid_dim_x\":        [\"MAX_GRID_DIM_X\"],\n",
    "        \"  max_grid_dim_y\":        [\"MAX_GRID_DIM_Y\"],\n",
    "        \"  max_grid_dim_z\":        [\"MAX_GRID_DIM_Z\"],\n",
    "        \"  max_shared_memory_per_block (bytes)\": [\"MAX_SHARED_MEMORY_PER_BLOCK\"],\n",
    "        \"  max_shared_memory_per_SM (bytes)\": [\"MAX_SHARED_MEMORY_PER_MULTIPROCESSOR\"],\n",
    "        \"  warp_size\":             [\"WARP_SIZE\"],\n",
    "        \"  compute_capability\":    [\"COMPUTE_CAPABILITY\", \"cc\"],\n",
    "    }\n",
    "    \n",
    "    feats = {}\n",
    "    label = \"  total_memory (bytes)\"\n",
    "    feats[label] = total\n",
    "    if show:\n",
    "        print(f\"{label:40}: {total}\")\n",
    "    for label, keys in props.items():\n",
    "        val = None\n",
    "        for k in keys:\n",
    "            val = getattr(dev, k, None)\n",
    "            feats[k] = val\n",
    "            if val is not None:\n",
    "                break\n",
    "        if show:\n",
    "            print(f\"{label:40}: {val}\")\n",
    "    return feats\n",
    "\n",
    "_ = device_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ Matrix multiplication pinned memory only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda,  float32\n",
    "import time\n",
    "\n",
    "# Controls threads per block and shared memory usmemAge.\n",
    "# The computation will be done on blocks of TILExTILE elements\n",
    "# TILE should not be larger than 32 in this example\n",
    "\n",
    "@cuda.jit\n",
    "def matMul_SMEM(A, B, C):\n",
    "    \"\"\"\n",
    "    Perform matrix multiplication of C = A * B using CUDA shared memory for improved performance. \n",
    "    Params:\n",
    "        A : 2D array\n",
    "            Input matrix A\n",
    "        B : 2D array\n",
    "            Input matrix B\n",
    "        C : 2D array\n",
    "            Output matrix C\n",
    "    \"\"\"\n",
    "    # Shared memory tiles\n",
    "    smemA = cuda.shared.array((TILE, TILE), dtype=float32)\n",
    "    smemB = cuda.shared.array((TILE, TILE), dtype=float32)\n",
    "\n",
    "    row, col = cuda.grid(2)   # (y, x) global indices\n",
    "    ty = cuda.threadIdx.y\n",
    "    tx = cuda.threadIdx.x\n",
    "\n",
    "    # Dimensions\n",
    "    M = A.shape[0]\n",
    "    P = A.shape[1]\n",
    "    N = B.shape[1]\n",
    "\n",
    "    if row >= M or col >= N:\n",
    "        return\n",
    "\n",
    "    acc = float32(0.0)\n",
    "\n",
    "    n_tiles = (P + TILE - 1) // TILE\n",
    "    for t in range(n_tiles):\n",
    "        kA = t * TILE + tx      # column index in A\n",
    "        kB = t * TILE + ty      # row index in B\n",
    "\n",
    "        # Load A tile\n",
    "        if kA < P:\n",
    "            smemA[ty, tx] = A[row, kA]\n",
    "        else:\n",
    "            smemA[ty, tx] = 0.0\n",
    "\n",
    "        # Load B tile\n",
    "        if kB < P:\n",
    "            smemB[ty, tx] = B[kB, col]\n",
    "        else:\n",
    "            smemB[ty, tx] = 0.0\n",
    "\n",
    "        # Synchronize to make sure the tiles are loaded\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Multiply the two tiles\n",
    "        for j in range(TILE):\n",
    "            acc += smemA[ty, j] * smemB[j, tx]\n",
    "\n",
    "        # Synchronize before loading the next tile\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    C[row, col] = acc\n",
    "\n",
    "        \n",
    "#  Matrix sizes:\n",
    "#     A: (N x P) float32\n",
    "#     B: (P x M) float32\n",
    "#     C: (N x M) float32\n",
    "\n",
    "TILE = 16  # Threads per block\n",
    "N = TILE *1000  # Number of rows\n",
    "M = TILE *1000  # Number of columns\n",
    "P = TILE *2000  # Inner dimension\n",
    "print(f\"Allocating host arrays for A({N}x{P}), B({P}x{M}), C({N}x{M})...\")\n",
    "A = np.ones((N,P), dtype=np.float32)        # A matrix \n",
    "B = 2* np.ones((P,M), dtype=np.float32)     # B matrix \n",
    "C = np.zeros((N, M), dtype=np.float32)      # Output matrix\n",
    "\n",
    "# GPU setup\n",
    "t0 = time.perf_counter()\n",
    "threads = (TILE, TILE)\n",
    "#                  cols                                     rows    \n",
    "blocks = ((M + (threads[0] - 1)) // threads[0], (N + (threads[1] - 1)) // threads[1])\n",
    "d_A = cuda.to_device(A)\n",
    "d_B = cuda.to_device(B)\n",
    "d_C = cuda.device_array((N, M), dtype=A.dtype)\n",
    "\n",
    "# launch kernels and time\n",
    "matMul_SMEM[blocks, threads](d_A, d_B, d_C)\n",
    "cuda.synchronize()\n",
    "\n",
    "# Final reduction on CPU\n",
    "C = d_C.copy_to_host()  # Final reduction in CPU\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "print(f\"Kernel blockParReduceSMEM execution time: {t1 - t0:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ÜòÔ∏è TODO..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tiled Matrix Multiplication on GPU**\n",
    "\n",
    "Given three matrices:\n",
    "-\t$A \\in \\mathbb{R}^{N \\times P}$\n",
    "-\t$B \\in \\mathbb{R}^{P \\times M}$\n",
    "-\t$C \\in \\mathbb{R}^{N \\times M}$\n",
    "\n",
    "stored in float32, compute:\n",
    "$$\n",
    "C = A \\cdot B\n",
    "$$\n",
    "using CUDA with shared memory tiling\n",
    "\n",
    "\n",
    "Tasks\n",
    "1.\tKernel implementation: Implement a CUDA kernel using shared memory tiles of size TILE √ó TILE that computes matrix multiplication.\n",
    "2.\tHost setup\n",
    "    -\tAllocate pinned host memory for A, B, and C.\n",
    "    -\tCopy A and B to the device.\n",
    "    -\tAllocate C on the device.\n",
    "3.\tKernel launch\n",
    "    -   Use a 2D grid and 2D blocks of size (TILE, TILE).\n",
    "    -\tEnsure correct bounds checking for non-multiple dimensions.\n",
    "4.\tTiming\n",
    "    - Measure kernel execution time.\n",
    "-   Optionally include host‚Äìdevice transfer time.\n",
    "5.\tValidation\n",
    "    - For $A = 1$ and $B = 2$, verify that $C_{ij} \\approx 2P$\n",
    "for all valid indices\n",
    "\n",
    "\n",
    "\n",
    "- Experiments\n",
    "\t-\tVary TILE = 8, 16, 32.\n",
    "\t-\tMeasure execution time and discuss performance differences.\n",
    "\t-\tIdentify the tile size that gives the best performance on your GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvmApCF76YD0"
   },
   "source": [
    "# ‚úÖ Tabular a function with streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ÜòÔ∏è TODO..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Given a 1D array $x ‚àà R‚Åø$ (float32), uniformly sampled in $[0, 2\\pi]$\n",
    "\n",
    "Compute\n",
    "$$\n",
    "y_i = \\sqrt{|\\sin^2(x_i) - \\cos^2(x_i)|}, \\quad i = 1,\\dots,n\n",
    "$$\n",
    "\n",
    "\n",
    "1Ô∏è‚É£ Create the domain (host)\n",
    "\n",
    "```\n",
    "import numpy as np, math\n",
    "n = 64 * 1024 * 1024\n",
    "x_host = np.linspace(0, 2*math.pi, n, dtype=np.float32)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "2Ô∏è‚É£ Allocate pinned host memory\n",
    "```\n",
    "from numba import cuda\n",
    "x_pinned = cuda.pinned_array(n, np.float32)\n",
    "y_pinned = cuda.pinned_array(n, np.float32)\n",
    "x_pinned[:] = x_host\n",
    "```\n",
    "\n",
    "3Ô∏è‚É£ CUDA kernel\n",
    "```\n",
    "@cuda.jit\n",
    "def tabular_xy(d_x, d_y, n):\n",
    "    i = cuda.grid(1)\n",
    "    if i < n:\n",
    "        s = math.sin(d_x[i])\n",
    "        c = math.cos(d_x[i])\n",
    "        d_y[i] = math.sqrt(abs(s*s - c*c))\n",
    "```\n",
    "\n",
    "4Ô∏è‚É£ Baseline (single stream)\n",
    "```\n",
    "d_x = cuda.to_device(x_pinned)\n",
    "d_y = cuda.device_array(n, np.float32)\n",
    "\n",
    "threads = 256\n",
    "blocks = (n + threads - 1) // threads\n",
    "\n",
    "tabular_xy[blocks, threads](d_x, d_y, n)\n",
    "cuda.synchronize()\n",
    "y_seq = d_y.copy_to_host()\n",
    "```\n",
    "\n",
    "**Measure total time and validate results.**\n",
    "\n",
    "\n",
    "5Ô∏è‚É£ Multi-stream execution\n",
    "\n",
    "\n",
    "**V1 ‚Äì per-stream pipeline**\n",
    "1.\tCreate streams\n",
    "2.\tFor each chunk:\n",
    "    -\tCopy chunk to device (H2D)\n",
    "    -\tLaunch kernel\n",
    "    -\tCopy chunk back to host (D2H)\n",
    "\n",
    "**V2 ‚Äì staged pipeline**\n",
    "1.\tCopy all chunks (H2D)\n",
    "2.\tLaunch all kernels\n",
    "3.\tCopy all chunks back (D2H)\n",
    "\n",
    "\n",
    "\n",
    "6Ô∏è‚É£ Experiments\n",
    "-\tVary n_streams = 1,2,4,8\n",
    "-\tVary threads = 128,256,512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ 1D convolution using pinned memory + streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ÜòÔ∏è TODO..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> üîπ What to use\n",
    "\n",
    "- A shared-memory kernel: conv1d_shared(out, x, mask, n)\n",
    "\n",
    "- A mask constructor: moving_avg_mask(radius)\n",
    "\n",
    "- A multi-stream host pipeline: conv1d_streams(h_x, radius, chunk, nstreams)\n",
    "\n",
    "<br> üîπ Setup Constants\n",
    "\n",
    "TPB = 256\n",
    "MAX_RADIUS = 512\n",
    "MAX_MASK = 2 * MAX_RADIUS + 1\n",
    "TILE = TPB + MAX_MASK - 1\n",
    "\n",
    "Constraints:\n",
    "\n",
    "radius <= MAX_RADIUS\n",
    "\n",
    "radius < TPB\n",
    "\n",
    "<br> üîπ Step 1 ‚Äî Create the Mask (Moving Average)\n",
    "\n",
    "def moving_avg_mask(radius: int):\n",
    "    # size = 2*radius + 1\n",
    "    # return np.full(size, 1.0/size, np.float32)\n",
    "\n",
    "<br> üîπ Step 5 ‚Äî Chunking Strategy\n",
    "\n",
    "Split the output into center chunks of size chunk, each extended by radius on both sides.\n",
    "\n",
    "<br> üîπ Step 6 ‚Äî Extended Chunk Bounds\n",
    "\n",
    "Compute ext_start, ext_end, ext_len, and halo_left correctly.\n",
    "\n",
    "<br> üîπ Step 7 ‚Äî Create Streams\n",
    "\n",
    "streams = [cuda.stream() for _ in range(nstreams)]\n",
    "\n",
    "<br> üîπ Step 8 ‚Äî Allocate Pinned Host Buffers\n",
    "\n",
    "pin_in  = [cuda.pinned_array(ext_max, np.float32) for _ in range(nstreams)]\n",
    "pin_out = [cuda.pinned_array(chunk,   np.float32) for _ in range(nstreams)]\n",
    "\n",
    "<br> üîπ Step 9 ‚Äî Allocate Device Buffers\n",
    "\n",
    "d_in  = [cuda.device_array(ext_max, np.float32) for _ in range(nstreams)]\n",
    "d_out = [cuda.device_array(ext_max, np.float32) for _ in range(nstreams)]\n",
    "\n",
    "<br> üîπ Step 10 ‚Äî Stream Pipeline Loop\n",
    "\n",
    "Schedule async H2D ‚Üí kernel ‚Üí async D2H on each stream.\n",
    "\n",
    "<br> üîπ Step 11 ‚Äî Synchronize-on-Reuse\n",
    "\n",
    "Before reusing a stream, synchronize and commit its pinned output to the final array.\n",
    "\n",
    "<br> üîπ Step 12 ‚Äî Drain Remaining Streams\n",
    "\n",
    "Synchronize remaining streams and copy results back.\n",
    "\n",
    "<br> üîπ Step 13 ‚Äî Correctness Check\n",
    "\n",
    "Compare GPU output with a CPU reference on a small test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ Transfer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer_benchmark.py\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "# --------------------\n",
    "# Parameters (adjustable)\n",
    "# --------------------\n",
    "TILE = 16\n",
    "N = TILE * 1000   # rows of A / C\n",
    "P = TILE * 2000   # inner dim\n",
    "M = TILE * 1000   # cols of B / C\n",
    "\n",
    "dtype = np.float32\n",
    "\n",
    "# size in bytes for reporting\n",
    "def bytes_of(arr):\n",
    "    return arr.nbytes\n",
    "\n",
    "# --------------------\n",
    "# Helper measurement functions\n",
    "# --------------------\n",
    "def time_h2d(host_arr, use_stream=False):\n",
    "    \"\"\"\n",
    "    Measure host -> device transfer time.\n",
    "    If use_stream True, perform an async to_device on a stream and synchronize the stream.\n",
    "    Returns seconds and device array (returned device array to be used later for D2H test).\n",
    "    \"\"\"\n",
    "    if use_stream:\n",
    "        s = cuda.stream()\n",
    "        t0 = time.perf_counter()\n",
    "        d = cuda.to_device(host_arr, stream=s)    # async when host_arr is pinned\n",
    "        s.synchronize()                            # ensure transfer complete for measurement\n",
    "        t1 = time.perf_counter()\n",
    "    else:\n",
    "        t0 = time.perf_counter()\n",
    "        d = cuda.to_device(host_arr)               # synchronous copy\n",
    "        t1 = time.perf_counter()\n",
    "    return (t1 - t0), d\n",
    "\n",
    "def time_d2h(device_arr, host_out, use_stream=False):\n",
    "    \"\"\"\n",
    "    Measure device -> host transfer time.\n",
    "    host_out must be a host array with correct shape/dtype (pinned if desired).\n",
    "    If use_stream True, perform async copy on a stream and synchronize the stream.\n",
    "    Returns seconds.\n",
    "    \"\"\"\n",
    "    if use_stream:\n",
    "        s = cuda.stream()\n",
    "        t0 = time.perf_counter()\n",
    "        device_arr.copy_to_host(host_out, stream=s)   # async if host_out pinned\n",
    "        s.synchronize()\n",
    "        t1 = time.perf_counter()\n",
    "    else:\n",
    "        t0 = time.perf_counter()\n",
    "        host_tmp = device_arr.copy_to_host()           # synchronous copy (returns new array)\n",
    "        # copy into host_out to keep consistent memory object (timing mainly device->host)\n",
    "        host_out[:] = host_tmp\n",
    "        t1 = time.perf_counter()\n",
    "    return (t1 - t0)\n",
    "\n",
    "def print_bytes(b):\n",
    "    kb = b / 1024.0\n",
    "    mb = kb / 1024.0\n",
    "    return f\"{b} B ({kb:.1f} KB / {mb:.3f} MB)\"\n",
    "\n",
    "# --------------------\n",
    "# Test scenario runner\n",
    "# --------------------\n",
    "def run_transfer_tests(shape, dtype=np.float32):\n",
    "    n_elems = shape[0]*shape[1]\n",
    "    print(f\"\\nArray shape: {shape}, elements: {n_elems:,}, dtype: {dtype}\")\n",
    "\n",
    "    # 1) Unpinned host array\n",
    "    host_unpinned = np.ones(shape, dtype=dtype)\n",
    "\n",
    "    # 2) Pinned host array (allocate pinned and write into it in-place)\n",
    "    host_pinned = cuda.pinned_array(shape, dtype=dtype)\n",
    "    host_pinned[:] = 2.0  # fill pinned array\n",
    "\n",
    "    print(\"Memory sizes (host):\")\n",
    "    print(\"  unpinned:\", print_bytes(bytes_of(host_unpinned)))\n",
    "    print(\"  pinned  :\", print_bytes(bytes_of(host_pinned)))\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Warm-up JIT + tiny transfers to avoid measuring first-call overheads\n",
    "    _ = cuda.to_device(np.ones((4,4), dtype=dtype))\n",
    "    cuda.synchronize()\n",
    "\n",
    "    # ---------- H2D (synchronous) ----------\n",
    "    t_up_unpinned, d_unpinned = time_h2d(host_unpinned, use_stream=False)\n",
    "    t_up_pinned, d_pinned = time_h2d(host_pinned, use_stream=False)\n",
    "\n",
    "    results['H2D_sync_unpinned'] = t_up_unpinned\n",
    "    results['H2D_sync_pinned'] = t_up_pinned\n",
    "    print(f\"\\nH2D synchronous: unpinned = {t_up_unpinned:.6f} s, pinned = {t_up_pinned:.6f} s, speedup = {t_up_unpinned / t_up_pinned if t_up_pinned>0 else float('inf'):.2f}x\")\n",
    "\n",
    "    # ---------- D2H (synchronous) ----------\n",
    "    # Prepare host output arrays of same dtype/shape (unpinned and pinned)\n",
    "    host_out_unpinned = np.empty(shape, dtype=dtype)\n",
    "    host_out_pinned = cuda.pinned_array(shape, dtype=dtype)\n",
    "\n",
    "    # Copy device arrays we created earlier back\n",
    "    t_down_unpinned = time_d2h(d_unpinned, host_out_unpinned, use_stream=False)\n",
    "    t_down_pinned = time_d2h(d_pinned, host_out_pinned, use_stream=False)\n",
    "\n",
    "    results['D2H_sync_unpinned'] = t_down_unpinned\n",
    "    results['D2H_sync_pinned'] = t_down_pinned\n",
    "    print(f\"\\nD2H synchronous: unpinned = {t_down_unpinned:.6f} s, pinned = {t_down_pinned:.6f} s, speedup = {t_down_unpinned / t_down_pinned if t_down_pinned>0 else float('inf'):.2f}x\")\n",
    "\n",
    "    # ---------- H2D (async on stream) ----------\n",
    "    t_up_unpinned_s, d_unpinned_s = time_h2d(host_unpinned, use_stream=True)\n",
    "    t_up_pinned_s, d_pinned_s = time_h2d(host_pinned, use_stream=True)\n",
    "\n",
    "    results['H2D_async_unpinned'] = t_up_unpinned_s\n",
    "    results['H2D_async_pinned'] = t_up_pinned_s\n",
    "    print(f\"\\nH2D async (stream): unpinned = {t_up_unpinned_s:.6f} s, pinned = {t_up_pinned_s:.6f} s\")\n",
    "\n",
    "    # ---------- D2H (async on stream) ----------\n",
    "    # allocate host_out buffers for async D2H\n",
    "    host_out_unpinned2 = np.empty(shape, dtype=dtype)\n",
    "    host_out_pinned2 = cuda.pinned_array(shape, dtype=dtype)\n",
    "\n",
    "    t_down_unpinned_s = time_d2h(d_unpinned_s, host_out_unpinned2, use_stream=True)\n",
    "    t_down_pinned_s = time_d2h(d_pinned_s, host_out_pinned2, use_stream=True)\n",
    "\n",
    "    results['D2H_async_unpinned'] = t_down_unpinned_s\n",
    "    results['D2H_async_pinned'] = t_down_pinned_s\n",
    "    print(f\"\\nD2H async (stream): unpinned = {t_down_unpinned_s:.6f} s, pinned = {t_down_pinned_s:.6f} s\")\n",
    "\n",
    "    # ---------- Overlap test (H2D + kernel + D2H) ----------\n",
    "    # A small kernel that just touches memory (to simulate compute)\n",
    "    @cuda.jit\n",
    "    def touch_kernel(x, y):\n",
    "        i, j = cuda.grid(2)\n",
    "        if i < x.shape[0] and j < x.shape[1]:\n",
    "            y[i, j] = x[i, j] * 2.0\n",
    "\n",
    "    # Use pinned buffers + streams to try to overlap copy<->compute\n",
    "    s = cuda.stream()\n",
    "    host_src = cuda.pinned_array(shape, dtype=dtype)\n",
    "    host_src[:] = 1.0\n",
    "    host_dst = cuda.pinned_array(shape, dtype=dtype)\n",
    "\n",
    "    # allocate device arrays reusing device array shape\n",
    "    d_tmp = cuda.device_array(shape, dtype=dtype)\n",
    "    d_out = cuda.device_array(shape, dtype=dtype)\n",
    "\n",
    "    # launch: async H2D, then kernel on stream, then async D2H; measure total wall time\n",
    "    t0 = time.perf_counter()\n",
    "    cuda.to_device(host_src, stream=s)           # async H2D into new device array\n",
    "    # kernel (on same stream) - determine blocks/threads\n",
    "    threads = (16, 16)\n",
    "    blocks = ((shape[0] + threads[0] - 1) // threads[0],\n",
    "              (shape[1] + threads[1] - 1) // threads[1])\n",
    "    touch_kernel[blocks, threads, s](d_tmp, d_out)\n",
    "    d_out.copy_to_host(host_dst, stream=s)      # async D2H\n",
    "    s.synchronize()\n",
    "    t1 = time.perf_counter()\n",
    "    overlap_time = t1 - t0\n",
    "    print(f\"\\nPinned overlap scenario (H2D async + kernel + D2H async) wall time: {overlap_time:.6f} s\")\n",
    "\n",
    "    # same sequence but **without** pinned host arrays -> likely no overlap\n",
    "    s2 = cuda.stream()\n",
    "    host_src2 = np.ones(shape, dtype=dtype)     # unpinned\n",
    "    host_dst2 = np.empty(shape, dtype=dtype)\n",
    "    t0 = time.perf_counter()\n",
    "    cuda.to_device(host_src2, stream=s2)        # generally synchronous or staged -> blocks\n",
    "    touch_kernel[blocks, threads, s2](d_tmp, d_out)\n",
    "    d_out.copy_to_host(host_dst2, stream=s2)\n",
    "    s2.synchronize()\n",
    "    t1 = time.perf_counter()\n",
    "    no_overlap_time = t1 - t0\n",
    "    print(f\"Unpinned overlap scenario wall time: {no_overlap_time:.6f} s\")\n",
    "\n",
    "    results['overlap_pinned'] = overlap_time\n",
    "    results['overlap_unpinned'] = no_overlap_time\n",
    "\n",
    "    return results\n",
    "\n",
    "# --------------------\n",
    "# Run tests for the matrices described\n",
    "# --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"TRANSFER BENCHMARK (H2D / D2H) ‚Äî pinned vs unpinned\\n\")\n",
    "    shape_A = (N, P)\n",
    "    shape_B = (P, M)\n",
    "    # We test with one large array shape; you can test both A and B separately\n",
    "    print(\"Testing transfer behavior for array A (N x P):\")\n",
    "    resA = run_transfer_tests(shape_A, dtype)\n",
    "    print(\"\\nTesting transfer behavior for array B (P x M):\")\n",
    "    resB = run_transfer_tests(shape_B, dtype)\n",
    "\n",
    "    # Nice summary\n",
    "    print(\"\\nSUMMARY (A):\")\n",
    "    for k, v in resA.items():\n",
    "        print(f\"  {k:25s}: {v:.6f} s\")\n",
    "    print(\"\\nSUMMARY (B):\")\n",
    "    for k, v in resB.items():\n",
    "        print(f\"  {k:25s}: {v:.6f} s\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "NO_C5o9-xRF_",
    "DvmApCF76YD0",
    "SbQlRthtJTQE"
   ],
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
